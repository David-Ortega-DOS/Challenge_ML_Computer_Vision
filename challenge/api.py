import logging
from pathlib import Path

import cv2
import fastapi
import numpy as np
from fastapi import File, Form, HTTPException, UploadFile
from ultralytics import YOLO

from .schemas import Detection, PredictionResponse
from .utils import iou_xyxy

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

logger = logging.getLogger(__name__)

ROOT_DIR = Path(__file__).resolve().parent.parent
ONNX_PATH = ROOT_DIR / "artifacts" / "model.onnx"
PT_PATH = ROOT_DIR / "artifacts" / "model_best.pt"

IMGSZ = 896
CONF_TH = 0.25
IOU_TH = 0.5
DEVICE = "cpu"

MODEL = None
CLASSES_META = {"names": ["error"]}

logger.info("[DEBUG] Starting model loading...")
logger.info(f"[DEBUG] ONNX_PATH: {ONNX_PATH}")
logger.info(f"[DEBUG] PT_PATH: {PT_PATH}")
logger.info(f"[DEBUG] Configured device: {DEVICE}")

try:

    if PT_PATH.exists():
        try:
            MODEL = YOLO(str(PT_PATH))
            logger.info("Model loaded: PyTorch (.pt) forced to CPU.")
        except Exception as e:
            logger.info(f"Failed to load PT: {e}")
            MODEL = None
    if MODEL is None and ONNX_PATH.exists():
        try:
            MODEL = YOLO(str(ONNX_PATH))
            logger.info("Model loaded: ONNX for fast inference (FALLBACK).")
        except Exception as e:
            logger.info(f"Failed to load ONNX: {e}. Discarding ONNX model.")
            MODEL = None

    elif MODEL is None:
        logger.info("[DEBUG] PyTorch (.pt) file not found.")

    if MODEL is None:
        raise FileNotFoundError("No valid model artifact could be loaded.")

    CLASSES_META["names"] = MODEL.names

    if len(CLASSES_META["names"]) != 17:
        logger.info(
            f"[WARNING] Model loaded with {len(CLASSES_META['names'])} classes, were expected 17."
        )
        logger.info(f"[DEBUG] Model classes: {CLASSES_META['names']}")
    else:
        logger.info("[DEBUG] Model loaded with the expected number of classes (17).")


except Exception as e:
    MODEL = None
    CLASSES_META = {"names": ["error"]}
    logger.info(f" FATAL Error while starting the API: {e}")
    logger.info("The API will start, but /predict will fail (Error 503).")

app = fastapi.FastAPI()


@app.get("/health", status_code=200)
async def get_health() -> dict:
    if MODEL is None:
        raise HTTPException(
            status_code=503, detail="Model artifact missing or failed to load."
        )

    return {
        "status": "model_loaded",
    }


@app.post("/predict", status_code=200, response_model=PredictionResponse)
async def post_predict(
    file: UploadFile = File(...), label: UploadFile = File(None)
) -> PredictionResponse:
    if MODEL is None:
        raise HTTPException(
            status_code=503, detail="Model is not loaded. Cannot run inference."
        )

    image_bytes = await file.read()
    label_data = None

    if label is not None:
        label_data = await label.read()
        logger.info(f"[DEBUG] Label received: {label.filename}")

    try:
        nparr = np.frombuffer(image_bytes, np.uint8)
        img_bgr = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

        if img_bgr is None:
            raise ValueError("Invalid image file or unsupported format.")

        # Par√°metros de inferencia
        logger.info("[DEBUG] Inference parameters:")
        logger.info(f"  CONF_TH: {CONF_TH}")
        logger.info(f"  IOU_TH: {IOU_TH}")
        logger.info(f"  IMGSZ: {IMGSZ}")

        # Verificar el formato de la imagen procesada
        logger.info("[DEBUG] Processing image for inference...")
        logger.info(
            f"  Received image size: {img_bgr.shape if img_bgr is not None else 'None'}"
        )
        logger.info(f"[DEBUG] Processed image: {file.filename}")

        # Depurar las detecciones generadas por el modelo
        logger.info("[DEBUG] Running inference with the model...")
        results_list = MODEL.predict(
            source=img_bgr, conf=CONF_TH, iou=IOU_TH, imgsz=IMGSZ, verbose=False
        )

        if results_list:
            results = results_list[0]
            if results.boxes is not None:
                logger.info("[DEBUG] Detections generated by the model:")
                for box, conf, cls_id in zip(
                    results.boxes.xyxy.cpu().numpy(),
                    results.boxes.conf.cpu().numpy(),
                    results.boxes.cls.cpu().numpy().astype(int),
                ):
                    logger.info(f"  Class: {cls_id}, Confidence: {conf}, Box: {box}")
            else:
                logger.info("[DEBUG] No detection boxes generated.")
        else:
            logger.info("[DEBUG] No inference results generated.")

    except Exception as e:
        logger.info(f"Error during image processing or YOLO inference: {e}")
        raise HTTPException(
            status_code=500, detail="Inference failed after image decoding."
        )

    detections = []

    if results_list:
        results = results_list[0]

        if results.boxes is not None:
            boxes = results.boxes.xyxy.cpu().numpy()
            confidences = results.boxes.conf.cpu().numpy()
            class_ids = results.boxes.cls.cpu().numpy().astype(int)

            num_classes = len(CLASSES_META["names"])

            for box, conf, cls_id in zip(boxes, confidences, class_ids):

                if 0 <= cls_id < num_classes:
                    detections.append(
                        Detection(
                            box=box.tolist(),
                            confidence=float(conf),
                            class_id=int(cls_id),
                            class_name=CLASSES_META["names"][cls_id],
                        )
                    )

    # Procesar etiquetas si se proporcionaron
    if label_data:
        logger.info("[DEBUG] Processing provided labels...")
        logger.info(f"[DEBUG] Content of the received label:\n{label_data.decode()}\n")
        labels = [
            list(map(float, line.strip().split()))
            for line in label_data.decode().splitlines()
        ]
        logger.info(f"[DEBUG] Number of labels received: {len(labels)}")

        # Obtener dimensiones de la imagen procesada
        H, W = img_bgr.shape[:2]

        # Comparar detecciones con etiquetas
        matched = 0
        total = len(labels)
        matched_labels = set()

        for label in labels:
            cls_id, cx, cy, w, h = label

            # Convertir las cajas de las etiquetas a coordenadas absolutas
            gt_box = [
                (cx - w / 2) * W,
                (cy - h / 2) * H,  # x_min, y_min
                (cx + w / 2) * W,
                (cy + h / 2) * H,  # x_max, y_max
            ]

            logger.info(
                f"[DEBUG] Processed label (absolute): Class {cls_id}, Box: {gt_box}"
            )

            best_iou = 0.0
            for det in detections:
                det_box = det.box
                iou = iou_xyxy(det_box, gt_box)
                logger.info(f"[DEBUG] Comparing with detection: {det_box}, IoU: {iou}")
                if iou >= IOU_TH and tuple(gt_box) not in matched_labels:
                    best_iou = max(best_iou, iou)

            if best_iou >= IOU_TH:
                matched += 1
                matched_labels.add(tuple(gt_box))

        recall = matched / total if total > 0 else 0.0
        logger.info(f"[DEBUG] Recall@IoU>={IOU_TH}: {recall:.3f} ({matched}/{total})")

    return PredictionResponse(
        status="success", num_detections=len(detections), detections=detections
    )
